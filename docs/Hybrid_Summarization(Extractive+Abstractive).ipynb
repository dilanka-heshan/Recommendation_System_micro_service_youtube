{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4JghBHnfJpL",
        "outputId": "6bc0211f-3d4b-4c5f-abb9-1e6ff9860d39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching transcript for lOD_EE96jhM ...\n",
            "Transcript length (chars): 7390\n",
            "Total sentences: 65\n",
            "Selected extractive sentences count: 10\n",
            "Selected indices: [np.int64(5), np.int64(6), np.int64(7), np.int64(14), np.int64(23), np.int64(34), np.int64(39), np.int64(42), np.int64(48), np.int64(62)]\n",
            "\n",
            "--- BART input preview ---\n",
            " So a picture or a graph or a handwritten note that might contain some valuable information, but without a way to convert that visual data into a form the LLM understands, it's gonna be inaccessible, but this is where vision language models come in, or VLMs. So vision language models are multimodal. That means that they can take in text but they can also take in image files as well and interpret their meaning and then generate as a response a text based output. So here we have the model generate a natural language description of an image. So, vision language models, they don't just process images and text. And then the result of all of this is then output, which is a text output, whether that's answering a question or summarizing a document or completing a sentence, but vision language mode \n",
            "---\n",
            "\n",
            "\n",
            "--- Abstractive summary (BART) preview ---\n",
            " So a picture or a graph or a handwritten note that might contain some valuable information, but without a way to convert that visual data into a form the LLM understands, it's gonna be inaccessible. This is where vision language models come in, or VLMs. They can take in text but they can also take in image files as well and interpret their meaning and then generate as a response a text based output. In essence, a vision language model has extended an LLM by introducing a multi-modal tokenization pipeline, one that allows images to be represented in a way that text-based transformers can process natively. \n",
            "---\n",
            "\n",
            "Hybrid summary generated in 39.2s\n",
            "\n",
            "===== FINAL HYBRID SUMMARY =====\n",
            "\n",
            "So a picture or a graph or a handwritten note that might contain some valuable information, but without a way to convert that visual data into a form the LLM understands, it's gonna be inaccessible. This is where vision language models come in, or VLMs. They can take in text but they can also take in image files as well and interpret their meaning and then generate as a response a text based output. In essence, a vision language model has extended an LLM by introducing a multi-modal tokenization pipeline, one that allows images to be represented in a way that text-based transformers can process natively. So vision language models are multimodal. So here we have the model generate a natural language description of an image. So, vision language models, they don't just process images and text. And then the result of all of this is then output, which is a text output, whether that's answering a question or summarizing a document or completing a sentence, but vision language models introduce something new and that new thing that they introduce is an image input. So unlike an LLM which tokenizes words, the vision encoder processes images as high-dimensional numerical data. So our images are now vectors, but these vectors can't be fed into a large language model directly either.\n",
            "\n",
            "===== DETAILS =====\n",
            "Time (s): 39.191158056259155\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install -q youtube-transcript-api sentence-transformers transformers nltk\n",
        "\n",
        "\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "import torch\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import time\n",
        "\n",
        "def fetch_transcript_text(video_id):\n",
        "    \"\"\"\n",
        "    Return transcript text as a single string for a YouTube video id.\n",
        "\n",
        "    \"\"\"\n",
        "    try:\n",
        "            ytt = YouTubeTranscriptApi()\n",
        "            transcript = ytt.fetch(video_id)\n",
        "\n",
        "            texts = []\n",
        "            for item in transcript:\n",
        "                if isinstance(item, dict) and 'text' in item:\n",
        "                    texts.append(item['text'])\n",
        "                else:\n",
        "\n",
        "                    texts.append(getattr(item, \"text\", str(item)))\n",
        "            return \" \".join(texts)\n",
        "    except Exception as e2:\n",
        "            raise RuntimeError(f\"Failed to fetch transcript\")\n",
        "\n",
        "# SBERT models for embeddings / scoring\n",
        "sbert_model_name = \"all-MiniLM-L6-v2\"\n",
        "sbert = SentenceTransformer(sbert_model_name)\n",
        "\n",
        "# BART setup (abstractive)\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Scoring functions\n",
        "def compute_relevance_scores(sentences):\n",
        "    \"\"\"\n",
        "    Relevance score: cosine similarity of each sentence embedding to the mean document embedding.\n",
        "    Returns scores list matching sentences.\n",
        "    \"\"\"\n",
        "    embeds = sbert.encode(sentences, convert_to_tensor=True, show_progress_bar=False)\n",
        "    doc_embed = embeds.mean(dim=0, keepdim=True)\n",
        "    cos = util.cos_sim(embeds, doc_embed).squeeze().cpu().numpy()\n",
        "    # normalize to 0..1\n",
        "    cos = (cos - cos.min()) / (cos.max() - cos.min() + 1e-8)\n",
        "    return cos.tolist(), embeds\n",
        "\n",
        "def compute_coherence_scores(sentences, sent_embeds=None):\n",
        "    \"\"\"\n",
        "    Coherence score for each sentence: similarity with the previous sentence (0 for first sentence).\n",
        "    We return a list same length as sentences.\n",
        "    \"\"\"\n",
        "    if sent_embeds is None:\n",
        "        sent_embeds = sbert.encode(sentences, convert_to_tensor=True, show_progress_bar=False)\n",
        "    scores = [0.0] * len(sentences)\n",
        "    if len(sentences) < 2:\n",
        "        return scores\n",
        "    for i in range(1, len(sentences)):\n",
        "        sim = util.cos_sim(sent_embeds[i], sent_embeds[i-1]).item()\n",
        "        # normalize later combined with relevance; keep raw sim ([-1,1])\n",
        "        scores[i] = (sim + 1) / 2.0  # map to [0,1]\n",
        "    return scores\n",
        "\n",
        "# Select top-k extractive sentences using combined score\n",
        "def select_extractive(sentences, top_k=8, alpha=0.7):\n",
        "    \"\"\"\n",
        "    alpha: weight for relevance, (1-alpha) for coherence\n",
        "    Returns selected sentences and their original indices (kept in original order).\n",
        "    \"\"\"\n",
        "    relevance_scores, sent_embeds = compute_relevance_scores(sentences)\n",
        "    coherence_scores = compute_coherence_scores(sentences, sent_embeds)\n",
        "    final_scores = []\n",
        "    for r, c in zip(relevance_scores, coherence_scores):\n",
        "        final_scores.append(alpha * r + (1 - alpha) * c)\n",
        "    # pick top_k indices\n",
        "    import numpy as np\n",
        "    idx_sorted = np.argsort(final_scores)[::-1]\n",
        "    top_idx = sorted(idx_sorted[:min(top_k, len(sentences))])  # sort to keep original order\n",
        "    selected = [sentences[i] for i in top_idx]\n",
        "    return selected, top_idx, sent_embeds\n",
        "\n",
        "# Abstractive rewrite with BART (input is a concatenated string)\n",
        "def abstractive_rewrite(text, max_input_tokens=1024, max_output_tokens=500, min_output_tokens=120):\n",
        "    \"\"\"\n",
        "    Feed text to BART and return abstractive summary string.\n",
        "    Text will be truncated if needed by tokenizer.\n",
        "    \"\"\"\n",
        "    inputs = bart_tokenizer([text], max_length=max_input_tokens, truncation=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        summary_ids = bart_model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            num_beams=5,\n",
        "            length_penalty=1.0,\n",
        "            max_length=max_output_tokens,\n",
        "            min_length=min_output_tokens,\n",
        "            no_repeat_ngram_size=3,\n",
        "            early_stopping=True,\n",
        "        )\n",
        "    summary = bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "    return summary\n",
        "\n",
        "# 8) Hybrid fusion and deduplication\n",
        "def hybrid_fusion(extractive_sents, abstractive_summary, dedupe_threshold=0.82):\n",
        "    \"\"\"\n",
        "    Combine abstractive summary (split to sentences) with extractive sentences.\n",
        "    We dedupe by embedding similarity: if a candidate sentence is highly similar to one already taken, skip it.\n",
        "    Final ordering: we preserve original order for extractive sentences; abstractive sentences are placed first,\n",
        "    but duplicates removed.\n",
        "    \"\"\"\n",
        "    abs_sents = sent_tokenize(abstractive_summary)\n",
        "    all_candidates = []\n",
        "    # mark source type for potential weighting/debugging\n",
        "    for s in abs_sents:\n",
        "        all_candidates.append((\"abs\", s))\n",
        "    for s in extractive_sents:\n",
        "        all_candidates.append((\"ext\", s))\n",
        "\n",
        "    final = []\n",
        "    final_embeds = []\n",
        "    for src, s in all_candidates:\n",
        "        emb = sbert.encode(s, convert_to_tensor=True)\n",
        "        keep = True\n",
        "        for fe in final_embeds:\n",
        "            if util.cos_sim(emb, fe).item() >= dedupe_threshold:\n",
        "                keep = False\n",
        "                break\n",
        "        if keep:\n",
        "            final.append((src, s))\n",
        "            final_embeds.append(emb)\n",
        "    # produce final text: we can order by preferring extractive order after abstractive:\n",
        "\n",
        "    abs_final = [s for src, s in final if src == \"abs\"]\n",
        "    ext_final = [s for src, s in final if src == \"ext\"]\n",
        "    final_text = \" \".join(abs_final + ext_final)\n",
        "    return final_text\n",
        "\n",
        "# 9) Full pipeline function\n",
        "def summarize_youtube(video_id, top_k=8, alpha=0.7):\n",
        "    t0 = time.time()\n",
        "    print(f\"Fetching transcript for {video_id} ...\")\n",
        "    text = fetch_transcript_text(video_id)\n",
        "    print(\"Transcript length (chars):\", len(text))\n",
        "    # sentence tokenization\n",
        "    sentences = sent_tokenize(text)\n",
        "    print(\"Total sentences:\", len(sentences))\n",
        "\n",
        "    # extractive selection with coherence-aware scoring\n",
        "    selected_sents, selected_idx, sent_embeds = select_extractive(sentences, top_k=top_k, alpha=alpha)\n",
        "    print(\"Selected extractive sentences count:\", len(selected_sents))\n",
        "    print(\"Selected indices:\", selected_idx)\n",
        "\n",
        "    # prepare input for BART: join selected sentences (preserve order)\n",
        "    bart_input = \" \".join(selected_sents)\n",
        "    print(\"\\n--- BART input preview ---\\n\", bart_input[:800], \"\\n---\\n\")\n",
        "\n",
        "    # abstractive rewriting\n",
        "    abstractive_summary = abstractive_rewrite(bart_input)\n",
        "    print(\"\\n--- Abstractive summary (BART) preview ---\\n\", abstractive_summary[:800], \"\\n---\\n\")\n",
        "\n",
        "    # hybrid fusion\n",
        "    final_summary = hybrid_fusion(selected_sents, abstractive_summary)\n",
        "    t1 = time.time()\n",
        "\n",
        "    print(\"Hybrid summary generated in {:.1f}s\".format(t1 - t0))\n",
        "    return {\n",
        "        \"video_id\": video_id,\n",
        "        \"extractive_selected_sentences\": selected_sents,\n",
        "        \"abstractive_summary\": abstractive_summary,\n",
        "        \"final_summary\": final_summary,\n",
        "        \"time_sec\": t1 - t0\n",
        "    }\n",
        "\n",
        "\n",
        "VIDEO_ID = \"lOD_EE96jhM\"  # replace with your video id\n",
        "result = summarize_youtube(VIDEO_ID, top_k=10, alpha=0.7)\n",
        "\n",
        "print(\"\\n===== FINAL HYBRID SUMMARY =====\\n\")\n",
        "print(result[\"final_summary\"])\n",
        "print(\"\\n===== DETAILS =====\")\n",
        "print(\"Time (s):\", result[\"time_sec\"])\n"
      ]
    }
  ]
}