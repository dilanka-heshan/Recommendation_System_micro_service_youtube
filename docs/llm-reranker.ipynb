{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-03T05:44:11.748842Z","iopub.execute_input":"2025-08-03T05:44:11.749045Z","iopub.status.idle":"2025-08-03T05:44:12.772498Z","shell.execute_reply.started":"2025-08-03T05:44:11.749026Z","shell.execute_reply":"2025-08-03T05:44:12.771568Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\n\n\nfetched_videos= [\n    # --- NLP ---\n    {\"video_id\": \"v1\", \"title\": \"Transformers Explained Visually\", \"description\": \"Ever wonder how those amazing AI models work? This video breaks down **Transformer models** step-by-step with stunning animations and easy-to-understand diagrams. You'll finally get how they power everything from ChatGPT to Google Search!\", \"domain\": \"NLP\"},\n    {\"video_id\": \"v2\", \"title\": \"Prompt Engineering for LLMs: Master the Art of Talking to AI\", \"description\": \"Unlock the full potential of **ChatGPT, GPT-4, Claude**, and other large language models! Learn the secrets to crafting effective **prompts** that get precise, high-quality results every time. Stop guessing, start mastering!\", \"domain\": \"NLP\"},\n    {\"video_id\": \"v3\", \"title\": \"Build a Semantic Search Engine with FAISS & BGE Embeddings\", \"description\": \"Go beyond keyword search! Discover how to create a powerful **semantic search engine** that understands meaning, not just words. We'll dive into **FAISS** for ultra-fast similarity search and **BGE embeddings** for superior relevance.\", \"domain\": \"NLP\"},\n\n    # --- Computer Vision ---\n    {\"video_id\": \"v4\", \"title\": \"YOLOv8 Object Detection: Train Your Custom AI Model in Minutes!\", \"description\": \"Ready to build your own AI that can see? This **hands-on tutorial** shows you how to train a custom **object detection model** using the lightning-fast **YOLOv8**. Detect anything you want, from cars to cats!\", \"domain\": \"Computer Vision\"},\n    {\"video_id\": \"v5\", \"title\": \"Vision Transformers (ViT) Demystified: The Future of Image AI\", \"description\": \"Transformers aren't just for text anymore! Explore how **Vision Transformers (ViT)** are revolutionizing **image classification** and other computer vision tasks. Understand the breakthrough that changed the game for image AI.\", \"domain\": \"Computer Vision\"},\n    {\"video_id\": \"v6\", \"title\": \"Medical Image Segmentation with U-Net: A Practical Guide\", \"description\": \"Dive into the world of **medical imaging AI**! Learn how to perform precise **image segmentation** using the powerful **U-Net** architecture. This tutorial covers everything you need to get started with real-world applications.\", \"domain\": \"Computer Vision\"},\n\n    # --- AI Coding Tools ---\n    {\"video_id\": \"v7\", \"title\": \"Boost Your Coding Speed with GitHub Copilot in VSCode\", \"description\": \"Say goodbye to boilerplate code! See how **GitHub Copilot** can dramatically improve your **productivity** by intelligently autocompleting your code right inside **VSCode**. It's like having a pair programmer 24/7!\", \"domain\": \"AI Coding Tools\"},\n    {\"video_id\": \"v8\", \"title\": \"Build Production-Ready AI Apps with LangChain (Workflows & Chains!)\", \"description\": \"Take your LLM projects to the next level! This video guides you through building complex, **production-grade AI applications** using **LangChain**. Master workflows, chains, and agents to create truly intelligent systems.\", \"domain\": \"AI Coding Tools\"},\n    {\"video_id\": \"v9\", \"title\": \"Debug & Optimize LLM Pipelines with PromptFlow: A Complete Guide\", \"description\": \"Is your LLM app behaving unexpectedly? Learn how to effectively **track, debug, and optimize** your **LLM-based applications** using **PromptFlow**. Pinpoint issues, improve performance, and build more reliable AI.\", \"domain\": \"AI Coding Tools\"}\n]\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T05:46:47.108907Z","iopub.execute_input":"2025-08-03T05:46:47.109220Z","iopub.status.idle":"2025-08-03T05:46:47.116773Z","shell.execute_reply.started":"2025-08-03T05:46:47.109195Z","shell.execute_reply":"2025-08-03T05:46:47.115735Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"user_histories = {\n    \"user_nlp\": [\n        {\"video_id\": \"h1\", \"title\": \"BERT Embeddings for Text Similarity: Hands-on Tutorial\", \"description\": \"Learn how to leverage **BERT** to generate powerful **sentence embeddings** and efficiently find similar texts in your datasets. This practical guide covers everything from setup to application.\"},\n        {\"video_id\": \"h2\", \"title\": \"Introduction to Named Entity Recognition (NER) with spaCy & Hugging Face\", \"description\": \"Master the art of extracting crucial information like **names, places, and dates** from unstructured text using industry-standard libraries like **spaCy** and the latest **transformer models** from Hugging Face.\"},\n        {\"video_id\": \"h3\", \"title\": \"Boosting Information Retrieval with Large Language Models (LLMs)\", \"description\": \"Discover how cutting-edge **Large Language Models** are revolutionizing **document retrieval and ranking**. Understand the techniques to integrate LLMs for more intelligent and context-aware search systems.\"}\n    ],\n    \"user_cv\": [\n        {\"video_id\": \"h4\", \"title\": \"Build a Real-Time Face Recognition System with OpenCV & Deep Learning\", \"description\": \"Create a complete **end-to-end face recognition pipeline** from scratch. This tutorial dives into practical implementation using **OpenCV** and advanced **deep learning techniques** for robust face identification.\"},\n        {\"video_id\": \"h5\", \"title\": \"Transfer Learning with ResNet: Accelerating Image Classification\", \"description\": \"Unlock the power of **transfer learning**! Learn how to effectively utilize **pre-trained ResNet models** to achieve state-of-the-art performance on your custom **image classification tasks** with minimal data.\"},\n        {\"video_id\": \"h6\", \"title\": \"Deep Learning in Medical Imaging: CNNs for Radiology & Healthcare\", \"description\": \"Explore the impactful applications of **Convolutional Neural Networks (CNNs)** in the medical field. This video covers how deep learning is transforming **radiology, disease diagnosis**, and various other healthcare domains.\"}\n    ],\n    \"user_tools\": [\n        {\"video_id\": \"h7\", \"title\": \"How GitHub Copilot Works: An Inside Look at AI Code Generation\", \"description\": \"Demystify **GitHub Copilot**! Go behind the scenes to understand the **inner workings** of this powerful AI coding assistant and the **Codex-based language models** that power its impressive code generation capabilities.\"},\n        {\"video_id\": \"h8\", \"title\": \"Advanced LLM API Wrappers: Building Complex Chains & Agents with LangChain\", \"description\": \"Elevate your **LangChain** skills! This video focuses on constructing sophisticated **LLM API wrappers**, implementing intricate **chains with memory**, and developing intelligent **agents** for complex AI applications.\"},\n        {\"video_id\": \"h9\", \"title\": \"Debugging Prompt Failures in LLM Applications: A Troubleshooting Guide\", \"description\": \"Frustrated with inconsistent LLM outputs? This essential guide provides practical **tips and strategies** for **understanding, identifying, and fixing common prompt-related errors** in your Large Language Model applications.\"}\n    ]\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T05:47:30.610021Z","iopub.execute_input":"2025-08-03T05:47:30.611036Z","iopub.status.idle":"2025-08-03T05:47:30.618506Z","shell.execute_reply.started":"2025-08-03T05:47:30.610989Z","shell.execute_reply":"2025-08-03T05:47:30.617306Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"! pip install sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T05:48:31.259095Z","iopub.execute_input":"2025-08-03T05:48:31.259930Z","iopub.status.idle":"2025-08-03T05:50:06.501407Z","shell.execute_reply.started":"2025-08-03T05:48:31.259896Z","shell.execute_reply":"2025-08-03T05:50:06.500141Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.52.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.1)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from sentence_transformers import CrossEncoder\nimport pandas as pd\n\ndef rerank_videos_for_user(user_id, user_histories, fetched_videos):\n    # Step 1: Create pseudo-query from user's watch history\n    user_query = \" \".join([\n        f\"{video['title']}. {video['description']}\"\n        for video in user_histories[user_id]\n    ])\n\n    # Step 2: Create (query, document) pairs\n    reranker_input = [\n        (\n            user_query,\n            f\"{video['title']}. {video['description']}\"\n        )\n        for video in fetched_videos\n    ]\n\n    # Step 3: Load reranker model and predict scores\n    model = CrossEncoder(\"BAAI/bge-reranker-base\")\n    scores = model.predict(reranker_input)\n\n    # Step 4: Attach scores and rank videos\n    scored_videos = pd.DataFrame(fetched_videos)\n    scored_videos[\"score\"] = scores\n    ranked = scored_videos.sort_values(by=\"score\", ascending=False)\n\n    return ranked[[\"video_id\", \"title\", \"score\"]]\n\n# 🔍 Example usage:\nranked_for_cv_user = rerank_videos_for_user(\"user_cv\", user_histories, fetched_videos)\nprint(\"Top videos for Computer Vision\")\nprint(ranked_for_cv_user.head(5))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T05:54:05.458540Z","iopub.execute_input":"2025-08-03T05:54:05.459510Z","iopub.status.idle":"2025-08-03T05:54:11.991539Z","shell.execute_reply.started":"2025-08-03T05:54:05.459482Z","shell.execute_reply":"2025-08-03T05:54:11.990689Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a157e065f9fe45ce92d09f71627cb656"}},"metadata":{}},{"name":"stdout","text":"Top videos for Computer Vision\n  video_id                                              title     score\n5       v6  Medical Image Segmentation with U-Net: A Pract...  0.072714\n2       v3  Build a Semantic Search Engine with FAISS & BG...  0.053502\n3       v4  YOLOv8 Object Detection: Train Your Custom AI ...  0.037500\n7       v8  Build Production-Ready AI Apps with LangChain ...  0.036063\n0       v1                    Transformers Explained Visually  0.034554\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# PairWise Reranking","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import CrossEncoder\nimport pandas as pd\nimport numpy as np\n\ndef rerank_videos_for_user_pairwise(user_id, user_histories, fetched_videos, agg='mean'):\n    model = CrossEncoder(\"BAAI/bge-reranker-base\")\n\n    # Prepare list to store all pairs and track which pair belongs to which fetched video\n    pairs = []\n    video_map = []  # to keep track of fetched_video index for each pair\n\n    for fetched_idx, fetched_video in enumerate(fetched_videos):\n        for user_video in user_histories[user_id]:\n            user_text = f\"{user_video['title']}. {user_video['description']}\"\n            fetched_text = f\"{fetched_video['title']}. {fetched_video['description']}\"\n            pairs.append((user_text, fetched_text))\n            video_map.append(fetched_idx)\n\n    # Predict scores for all pairs at once\n    scores = model.predict(pairs)\n\n    # Aggregate scores per fetched video\n    agg_scores = {}\n    counts = {}\n\n    for idx, score in enumerate(scores):\n        vid_idx = video_map[idx]\n        agg_scores.setdefault(vid_idx, 0)\n        counts.setdefault(vid_idx, 0)\n        agg_scores[vid_idx] += score\n        counts[vid_idx] += 1\n\n    # Calculate mean or max\n    if agg == 'mean':\n        final_scores = {vid_idx: agg_scores[vid_idx]/counts[vid_idx] for vid_idx in agg_scores}\n    elif agg == 'max':\n        # For max, we need to track max instead of sum; redo aggregation:\n        max_scores = {}\n        for idx, score in enumerate(scores):\n            vid_idx = video_map[idx]\n            if vid_idx not in max_scores or score > max_scores[vid_idx]:\n                max_scores[vid_idx] = score\n        final_scores = max_scores\n    else:\n        raise ValueError(\"Aggregation method must be 'mean' or 'max'\")\n\n    # Build DataFrame for results\n    results = []\n    for vid_idx, score in final_scores.items():\n        results.append({\n            \"video_id\": fetched_videos[vid_idx][\"video_id\"],\n            \"title\": fetched_videos[vid_idx][\"title\"],\n            \"score\": score\n        })\n\n    ranked = pd.DataFrame(results).sort_values(by=\"score\", ascending=False)\n    return ranked\n\n# Example usage:\nranked_for_cv_user = rerank_videos_for_user_pairwise(\"user_cv\", user_histories, fetched_videos, agg='mean')\nprint(\"Top videos for Computer Vision (pairwise):\")\nprint(ranked_for_cv_user.head(5))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T05:59:48.872123Z","iopub.execute_input":"2025-08-03T05:59:48.873108Z","iopub.status.idle":"2025-08-03T05:59:58.479268Z","shell.execute_reply.started":"2025-08-03T05:59:48.873077Z","shell.execute_reply":"2025-08-03T05:59:58.478376Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05d10f6e265b423886297c474f353a98"}},"metadata":{}},{"name":"stdout","text":"Top videos for Computer Vision (pairwise):\n  video_id                                              title     score\n5       v6  Medical Image Segmentation with U-Net: A Pract...  0.015356\n4       v5  Vision Transformers (ViT) Demystified: The Fut...  0.001214\n0       v1                    Transformers Explained Visually  0.001143\n1       v2  Prompt Engineering for LLMs: Master the Art of...  0.000728\n8       v9  Debug & Optimize LLM Pipelines with PromptFlow...  0.000322\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Embedding Based","metadata":{}},{"cell_type":"markdown","source":"This approach uses vector embeddings of video titles and descriptions to represent their semantic content. While embeddings are commonly used to measure similarity between texts, here we specifically compute similarity scores between each video in the user’s watch history and each candidate video individually. These pairwise similarity scores are then aggregated—using methods like mean or max—to derive an overall relevance score for each candidate video.","metadata":{}},{"cell_type":"markdown","source":"Even though the approach increases accuracy, it can be computationally intensive. \nSuggestions: Limit user history to recent k videos (e.g., k = 10)\n             Apply fast filtering first (e.g., using simple keyword match or ANN  search), and only rerank top N candidates with this method. (Can achieve this using Youtube data API)","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, util\nimport pandas as pd\nimport numpy as np\n\ndef embed_texts(videos, model, batch_size=32):\n    texts = [f\"{v['title']}. {v['description']}\" for v in videos]\n    embeddings = model.encode(texts, batch_size=batch_size, convert_to_tensor=True, show_progress_bar=False)\n    return embeddings\n\ndef rerank_videos_with_embeddings(user_id, user_histories, fetched_videos, agg=\"mean\"):\n    model = SentenceTransformer(\"BAAI/bge-base-en\")  \n\n    user_videos = user_histories[user_id]\n    \n    # Step 1: Embed user history and fetched videos\n    user_embeds = embed_texts(user_videos, model)\n    fetched_embeds = embed_texts(fetched_videos, model)\n\n    # Step 2: Compute cosine similarity matrix (user_history_size x fetched_size)\n    similarity_matrix = util.cos_sim(user_embeds, fetched_embeds).cpu().numpy()\n\n    # Step 3: Aggregate similarity scores for each fetched video\n    if agg == \"mean\":\n        agg_scores = similarity_matrix.mean(axis=0)\n    elif agg == \"max\":\n        agg_scores = similarity_matrix.max(axis=0)\n    else:\n        raise ValueError(\"agg must be 'mean' or 'max'\")\n\n    # Step 4: Rank videos by aggregated score\n    results = []\n    for i, score in enumerate(agg_scores):\n        results.append({\n            \"video_id\": fetched_videos[i][\"video_id\"],\n            \"title\": fetched_videos[i][\"title\"],\n            \"score\": score\n        })\n\n    ranked = pd.DataFrame(results).sort_values(by=\"score\", ascending=False)\n    return ranked\n\n# Example usage:\nranked_for_nlp_user = rerank_videos_with_embeddings(\"user_nlp\", user_histories, fetched_videos, agg=\"mean\")\nprint(\"Top videos for NLP:\")\nprint(ranked_for_nlp_user.head(5))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T06:06:04.187505Z","iopub.execute_input":"2025-08-03T06:06:04.187933Z","iopub.status.idle":"2025-08-03T06:06:06.469404Z","shell.execute_reply.started":"2025-08-03T06:06:04.187906Z","shell.execute_reply":"2025-08-03T06:06:06.468438Z"}},"outputs":[{"name":"stdout","text":"Top videos for NLP:\n  video_id                                              title     score\n2       v3  Build a Semantic Search Engine with FAISS & BG...  0.836045\n1       v2  Prompt Engineering for LLMs: Master the Art of...  0.835638\n8       v9  Debug & Optimize LLM Pipelines with PromptFlow...  0.798224\n4       v5  Vision Transformers (ViT) Demystified: The Fut...  0.797162\n7       v8  Build Production-Ready AI Apps with LangChain ...  0.796680\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"Using Reranker on top of filtered videos using embeddings","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, CrossEncoder, util\nimport pandas as pd\nimport numpy as np\n\ndef embed_texts(videos, model, batch_size=32):\n    texts = [f\"{v['title']}. {v['description']}\" for v in videos]\n    embeddings = model.encode(texts, batch_size=batch_size, convert_to_tensor=True, show_progress_bar=False)\n    return embeddings\n\ndef rerank_videos_with_embeddings_and_crossencoder(user_id, user_histories, fetched_videos, agg=\"mean\"):\n    embed_model = SentenceTransformer(\"BAAI/bge-base-en\")\n    rerank_model = CrossEncoder(\"BAAI/bge-reranker-base\")\n\n    user_videos = user_histories[user_id]\n\n    # Step 1: Embed user history and fetched videos\n    user_embeds = embed_texts(user_videos, embed_model)\n    fetched_embeds = embed_texts(fetched_videos, embed_model)\n\n    # Step 2: Compute cosine similarity matrix\n    similarity_matrix = util.cos_sim(user_embeds, fetched_embeds).cpu().numpy()\n\n    # Step 3: Aggregate similarity scores\n    if agg == \"mean\":\n        agg_scores = similarity_matrix.mean(axis=0)\n    elif agg == \"max\":\n        agg_scores = similarity_matrix.max(axis=0)\n    else:\n        raise ValueError(\"agg must be 'mean' or 'max'\")\n\n    # Step 4: Build video list with aggregated scores\n    candidates = []\n    for i, score in enumerate(agg_scores):\n        candidates.append({\n            \"video_id\": fetched_videos[i][\"video_id\"],\n            \"title\": fetched_videos[i][\"title\"],\n            \"description\": fetched_videos[i][\"description\"],\n            \"agg_score\": score\n        })\n\n    # Step 5: Build query from user history (concatenated titles+descriptions)\n    user_query = \" \".join([f\"{v['title']}. {v['description']}\" for v in user_videos])\n\n    # Step 6: Prepare input pairs for reranker\n    rerank_inputs = [(user_query, f\"{v['title']}. {v['description']}\") for v in candidates]\n    rerank_scores = rerank_model.predict(rerank_inputs)\n\n    # Step 7: Assign rerank scores and sort\n    for i, score in enumerate(rerank_scores):\n        candidates[i][\"final_score\"] = score\n\n    ranked = pd.DataFrame(candidates).sort_values(by=\"final_score\", ascending=False)\n    return ranked\n\n# Example usage:\nranked_for_nlp_user = rerank_videos_with_embeddings_and_crossencoder(\"user_nlp\", user_histories, fetched_videos, agg=\"mean\")\nprint(\"Top videos for NLP after reranking:\")\nprint(ranked_for_nlp_user[[\"video_id\", \"title\", \"final_score\"]].head(5))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T06:19:27.369222Z","iopub.execute_input":"2025-08-03T06:19:27.369595Z","iopub.status.idle":"2025-08-03T06:19:37.189141Z","shell.execute_reply.started":"2025-08-03T06:19:27.369573Z","shell.execute_reply":"2025-08-03T06:19:37.187314Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b22d5ce39c44497ab5aa1c1b00ace80"}},"metadata":{}},{"name":"stdout","text":"Top videos for NLP after reranking:\n  video_id                                              title  final_score\n4       v5  Vision Transformers (ViT) Demystified: The Fut...     0.362658\n2       v3  Build a Semantic Search Engine with FAISS & BG...     0.351487\n0       v1                    Transformers Explained Visually     0.296336\n1       v2  Prompt Engineering for LLMs: Master the Art of...     0.154787\n3       v4  YOLOv8 Object Detection: Train Your Custom AI ...     0.105344\n","output_type":"stream"}],"execution_count":13}]}